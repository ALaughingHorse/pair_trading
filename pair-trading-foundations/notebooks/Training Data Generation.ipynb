{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45bfe8ae-1745-4eaf-9d12-fe6dab5e6ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from pair_trading_foundations.data_generation import ExecutePairTrading, generate_training_data\n",
    "random.seed(23)\n",
    "import cProfile\n",
    "import pstats\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "from time import time\n",
    "\n",
    "def chunker(seq, size):\n",
    "    # split a list into chunks\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05f8ad47-14f9-4351-bf7d-15597793e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('Data/sp500_full_20181231_to_20231229.csv')\n",
    "data = pd.read_csv('Data/sp500_full_20150101_to_20191231.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7e176ad-efe6-4f46-a66e-52072e5d1de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_count_tb = data[['Ticker']].groupby('Ticker').size().reset_index()\n",
    "value_count_tb.columns = ['Ticker', 'Count']\n",
    "stock_to_keep = value_count_tb['Ticker'][value_count_tb.Count==value_count_tb.Count.max()]\n",
    "data = data[data.Ticker.isin(stock_to_keep)]\n",
    "sampled_tickers = random.sample(list(stock_to_keep.values), 10)\n",
    "# data_tech = data[data['GICS Sector'].isin(['Information Technology'])]\n",
    "data_sampled = data[data['Ticker'].isin(sampled_tickers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbaa0874-7dec-426a-a4dc-978a3d0fd855",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = list(set(data.Ticker.values))\n",
    "combinations = list(itertools.combinations(tickers, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6b8d625-9356-49a8-8f43-55bd6557bf27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113050"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffebc165-d731-4f53-80bd-eaafbc2097c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = list(chunker(combinations, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca6a1c0-f52c-40cb-85e2-44b7fefd1bfc",
   "metadata": {},
   "source": [
    "# Generate for all pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc64fa11-909b-413b-aadf-e01318df0ddc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting 1th out of 114 batches\n",
      "1000 stock pairs detected\n",
      "Took 0.1914989948272705 to initilize. Entering ticker pair loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoma/Desktop/current_desktop/mids/pair_trading/pair-trading-foundations/notebooks/pair_trading_foundations/data_generation.py:225: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  features_tb = pd.concat(\n",
      "/Users/xiaoma/Desktop/current_desktop/mids/pair_trading/pair-trading-foundations/notebooks/pair_trading_foundations/data_generation.py:271: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  labels_tb = pd.concat(\n",
      "/Users/xiaoma/Desktop/current_desktop/mids/pair_trading/pair-trading-foundations/notebooks/pair_trading_foundations/data_generation.py:280: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  pnl_metadata_tb = pd.concat(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the 100th pair\n",
      "Used 125.94107294082642 for the 100 pairs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m start_ts\u001b[38;5;241m=\u001b[39mtime()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGetting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mth out of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(batches)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m batches\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m features_tb, labels_tb, pnl_metadata_tb \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_training_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcalculate_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombinations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m combined \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(features_tb, labels_tb, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicker_P1\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicker_P2\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# combined = pd.merge(combined, pnl_metadata_tb[['Date', 'Ticker_P1','Ticker_P2', 'trade_executions']], how='left', on=['Date', 'Ticker_P1','Ticker_P2'])\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# combined = combined[combined.pnls.notnull()].reset_index(drop=True)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/current_desktop/mids/pair_trading/pair-trading-foundations/notebooks/pair_trading_foundations/data_generation.py:261\u001b[0m, in \u001b[0;36mgenerate_training_data\u001b[0;34m(data, training_len, test_len, calculate_label, verbose, execution_class, combinations)\u001b[0m\n\u001b[1;32m    259\u001b[0m             trading_tables\u001b[38;5;241m.\u001b[39mappend(result\u001b[38;5;241m.\u001b[39mtrade_execution_table)\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 261\u001b[0m             trading_tables\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    263\u001b[0m end_ts \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for batch in batches:\n",
    "    start_ts=time()\n",
    "    print(f'Getting {i+1}th out of {len(batches)} batches')\n",
    "    features_tb, labels_tb, pnl_metadata_tb = generate_training_data(\n",
    "        data=data,\n",
    "        training_len=300,\n",
    "        test_len=60,\n",
    "        calculate_label=True,\n",
    "        verbose=False,\n",
    "        combinations=batch\n",
    "    )\n",
    "    combined = pd.merge(features_tb, labels_tb, how='left', on=['Date', 'Ticker_P1','Ticker_P2']).reset_index(drop=True)\n",
    "    # combined = pd.merge(combined, pnl_metadata_tb[['Date', 'Ticker_P1','Ticker_P2', 'trade_executions']], how='left', on=['Date', 'Ticker_P1','Ticker_P2'])\n",
    "    # combined = combined[combined.pnls.notnull()].reset_index(drop=True)\n",
    "    combined.to_csv(f'Data/Training/pair_features{i+1}_300_60.csv', index=False)\n",
    "    end_ts = time()\n",
    "    print(f\"Took {end_ts - start_ts} seconds\")\n",
    "    i+=1\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2001dcb-b0ec-495a-8522-75da6babdf11",
   "metadata": {},
   "source": [
    "# Generate data for sample pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1865eb17-41ec-44b3-a696-c7b4c8b14e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 stock pairs detected\n",
      "Took 0.014698982238769531 to initilize. Entering ticker pair loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoma/Desktop/current_desktop/mids/pair_trading/pair-trading-foundations/notebooks/pair_trading_foundations/data_generation.py:225: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  features_tb = pd.concat(\n",
      "/Users/xiaoma/Desktop/current_desktop/mids/pair_trading/pair-trading-foundations/notebooks/pair_trading_foundations/data_generation.py:271: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  labels_tb = pd.concat(\n",
      "/Users/xiaoma/Desktop/current_desktop/mids/pair_trading/pair-trading-foundations/notebooks/pair_trading_foundations/data_generation.py:280: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  pnl_metadata_tb = pd.concat(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 27.972273349761963 to finish\n"
     ]
    }
   ],
   "source": [
    "features_tb, labels_tb, pnl_metadata_tb = generate_training_data(\n",
    "        data=data_sampled,\n",
    "        training_len=500,\n",
    "        test_len=120,\n",
    "        calculate_label=True,\n",
    "        verbose=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c14f98-ee57-49ab-99c8-0807baee5568",
   "metadata": {},
   "source": [
    "# Write data out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82e44077-ea59-47d3-8470-6fb15b6ec440",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.merge(features_tb, labels_tb, how='left', on=['Date', 'Ticker_P1','Ticker_P2']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0106cda4-3bb2-4fd3-abf1-40122feb4cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.merge(combined, pnl_metadata_tb[['Date', 'Ticker_P1','Ticker_P2', 'trade_executions']], how='left', on=['Date', 'Ticker_P1','Ticker_P2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44867a0b-987c-4561-b464-4950c920982f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Ticker_P1', 'Close_P1', 'Ticker_P2', 'Close_P2', 'High_P1',\n",
       "       'High_P2', 'Low_P1', 'Low_P2', 'Volume_P1', 'Volume_P2', 'abs_spread',\n",
       "       'abs_spread_mean', 'abs_spread_std', 'abs_spread_mean_l28',\n",
       "       'abs_spread_std_l28', 'spread_normed', 'abs_spread_normed_max',\n",
       "       'abs_spread_normed_90th', 'abs_spread_normed_75th',\n",
       "       'abs_spread_normed_median', 'abs_spread_normed_l7_avg',\n",
       "       'abs_spread_normed_l14_avg', 'cos_sim', 'corr_coef', 'pnls',\n",
       "       'num_entries', 'trade_executions'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffd20ab7-fc26-40b1-a739-1864714a874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/spotcheckout_output.pkl','wb') as file:\n",
    "    pickle.dump(combined, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a72d072-48e4-411c-a31d-f54459140fec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
