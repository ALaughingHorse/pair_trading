{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45bfe8ae-1745-4eaf-9d12-fe6dab5e6ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from pair_trading_foundations.data_generation import ExecutePairTrading, generate_training_data\n",
    "random.seed(23)\n",
    "import cProfile\n",
    "import pstats\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "from time import time\n",
    "\n",
    "def chunker(seq, size):\n",
    "    # split a list into chunks\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05f8ad47-14f9-4351-bf7d-15597793e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('Data/sp500_full_20181231_to_20231229.csv')\n",
    "data = pd.read_csv('Data/sp500_full_20150101_to_20191231.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7e176ad-efe6-4f46-a66e-52072e5d1de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_count_tb = data[['Ticker']].groupby('Ticker').size().reset_index()\n",
    "value_count_tb.columns = ['Ticker', 'Count']\n",
    "stock_to_keep = value_count_tb['Ticker'][value_count_tb.Count==value_count_tb.Count.max()]\n",
    "data = data[data.Ticker.isin(stock_to_keep)]\n",
    "sampled_tickers = random.sample(list(stock_to_keep.values), 10)\n",
    "# data_tech = data[data['GICS Sector'].isin(['Information Technology'])]\n",
    "data_sampled = data[data['Ticker'].isin(sampled_tickers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbaa0874-7dec-426a-a4dc-978a3d0fd855",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = list(set(data.Ticker.values))\n",
    "combinations = list(itertools.combinations(tickers, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6b8d625-9356-49a8-8f43-55bd6557bf27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113050"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffebc165-d731-4f53-80bd-eaafbc2097c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = list(chunker(combinations, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca6a1c0-f52c-40cb-85e2-44b7fefd1bfc",
   "metadata": {},
   "source": [
    "# Generate for all pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc64fa11-909b-413b-aadf-e01318df0ddc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting 1th out of 114 batches\n",
      "1000 stock pairs detected\n",
      "Took 0.15260100364685059 to initilize. Entering ticker pair loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoma/Desktop/current_desktop/mids/pair_trading/pair-trading-foundations/notebooks/pair_trading_foundations/data_generation.py:227: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  features_tb = pd.concat(\n",
      "/Users/xiaoma/Desktop/current_desktop/mids/pair_trading/pair-trading-foundations/notebooks/pair_trading_foundations/data_generation.py:273: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  labels_tb = pd.concat(\n",
      "/Users/xiaoma/Desktop/current_desktop/mids/pair_trading/pair-trading-foundations/notebooks/pair_trading_foundations/data_generation.py:282: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  pnl_metadata_tb = pd.concat(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the 100th pair\n",
      "Used 312.9467430114746 for the 100 pairs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m start_ts\u001b[38;5;241m=\u001b[39mtime()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGetting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mth out of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(batches)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m batches\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m features_tb, labels_tb, pnl_metadata_tb \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_training_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcalculate_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombinations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m combined \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(features_tb, labels_tb, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicker_P1\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicker_P2\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# combined = pd.merge(combined, pnl_metadata_tb[['Date', 'Ticker_P1','Ticker_P2', 'trade_executions']], how='left', on=['Date', 'Ticker_P1','Ticker_P2'])\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# combined = combined[combined.pnls.notnull()].reset_index(drop=True)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/current_desktop/mids/pair_trading/pair-trading-foundations/notebooks/pair_trading_foundations/data_generation.py:227\u001b[0m, in \u001b[0;36mgenerate_training_data\u001b[0;34m(data, training_len, test_len, calculate_label, verbose, execution_class, combinations)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_ts\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_ts\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to calculate features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m## Appending the tables\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m features_tb \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures_tb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeature_columns\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# Calculate the labels\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m calculate_label:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pair-trading-foundations-SdobHK0C-py3.11/lib/python3.11/site-packages/pandas/core/reshape/concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    383\u001b[0m     objs,\n\u001b[1;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    393\u001b[0m )\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pair-trading-foundations-SdobHK0C-py3.11/lib/python3.11/site-packages/pandas/core/reshape/concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[1;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[0;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/pair-trading-foundations-SdobHK0C-py3.11/lib/python3.11/site-packages/pandas/core/internals/concat.py:177\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m    167\u001b[0m vals \u001b[38;5;241m=\u001b[39m [ju\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m ju \u001b[38;5;129;01min\u001b[39;00m join_units]\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_extension:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# _is_uniform_join_units ensures a single dtype, so\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m#  we can use np.concatenate, which is more performant\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# expected \"Union[_SupportsArray[dtype[Any]],\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;66;03m# _NestedSequence[_SupportsArray[dtype[Any]]]]\"\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_1d_only_ea_dtype(blk\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     values \u001b[38;5;241m=\u001b[39m concat_compat(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ea_compat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for batch in batches:\n",
    "    start_ts=time()\n",
    "    print(f'Getting {i+1}th out of {len(batches)} batches')\n",
    "    features_tb, labels_tb, pnl_metadata_tb = generate_training_data(\n",
    "        data=data,\n",
    "        training_len=300,\n",
    "        test_len=120,\n",
    "        calculate_label=True,\n",
    "        verbose=False,\n",
    "        combinations=batch\n",
    "    )\n",
    "    combined = pd.merge(features_tb, labels_tb, how='left', on=['Date', 'Ticker_P1','Ticker_P2']).reset_index(drop=True)\n",
    "    # combined = pd.merge(combined, pnl_metadata_tb[['Date', 'Ticker_P1','Ticker_P2', 'trade_executions']], how='left', on=['Date', 'Ticker_P1','Ticker_P2'])\n",
    "    # combined = combined[combined.pnls.notnull()].reset_index(drop=True)\n",
    "    combined.to_csv(f'Data/Training/pair_features{i+1}_300_120.csv', index=False)\n",
    "    end_ts = time()\n",
    "    print(f\"Took {end_ts - start_ts} seconds\")\n",
    "    i+=1\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2001dcb-b0ec-495a-8522-75da6babdf11",
   "metadata": {},
   "source": [
    "# Generate data for sample pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1865eb17-41ec-44b3-a696-c7b4c8b14e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_tb, labels_tb, pnl_metadata_tb = generate_training_data(\n",
    "        data=data_sampled,\n",
    "        training_len=500,\n",
    "        test_len=120,\n",
    "        calculate_label=True,\n",
    "        verbose=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c14f98-ee57-49ab-99c8-0807baee5568",
   "metadata": {},
   "source": [
    "# Write data out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e44077-ea59-47d3-8470-6fb15b6ec440",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.merge(features_tb, labels_tb, how='left', on=['Date', 'Ticker_P1','Ticker_P2']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0106cda4-3bb2-4fd3-abf1-40122feb4cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.merge(combined, pnl_metadata_tb[['Date', 'Ticker_P1','Ticker_P2', 'trade_executions']], how='left', on=['Date', 'Ticker_P1','Ticker_P2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44867a0b-987c-4561-b464-4950c920982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd20ab7-fc26-40b1-a739-1864714a874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/spotcheckout_output.pkl','wb') as file:\n",
    "    pickle.dump(combined, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a72d072-48e4-411c-a31d-f54459140fec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
